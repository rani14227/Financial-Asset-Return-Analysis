{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wYST_arCeCj"
      },
      "source": [
        "# **Financial Asset & Return Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ays1bUdkCeCl"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import yfinance as yfin\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QroHwiJaCeCm",
        "tags": []
      },
      "source": [
        "## **1. Obtaining and Transforming Financial Data**\n",
        "\n",
        "### **1.1 Importing Data**\n",
        "Since this course has a more practical focus, we will start by pulling price data into Python and showing how to simply clean and transform the data so it's in a format that is easier to work with.  \n",
        "\n",
        "We use the `pandas_datareader` library in order to pull financial data. We will pull data from the FRED (Federal Reserve Economic Data) database. Keep in mind that we can use the `pandas_datareader` package to pull from many different sources, like OECD, Yahoo Finance, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkY3wtBQCeCm"
      },
      "outputs": [],
      "source": [
        "# Starting and end dates\n",
        "start = datetime.date(2019, 8, 1)\n",
        "end = datetime.date(2024, 8, 1)\n",
        "\n",
        "# Get data\n",
        "df = yfin.download([\"^GSPC\", \"^IXIC\", \"BTC-USD\"], start, end, auto_adjust = False)['Adj Close']\n",
        "\n",
        "# Display the first five rows\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdYmDPbeCeCm"
      },
      "source": [
        "We've retrieved the data from the two most popular U.S. indices, the NASDAQ and S&P 500, along with the daily Bitcoin prices from the last five years. In this lesson, some basic data cleaning will be done by removing nulls (weekend data). Then, we'll have a DataFrame containing the dates and prices of three different assets. We need to compare returns instead of prices here for a couple of reasons:\n",
        "\n",
        "1. Return is a scale-free summary of an investment opportunity.\n",
        "2. Returns have statistical properties that are easier to work with. (This will be discussed more in this lesson.)  \n",
        "\n",
        "The next question is whether to use simple returns or log returns. Using the following variables, we can define the different types of return:\n",
        "$p_{_1}$ = final value, $p_{_0}$ = initial value.\n",
        "\n",
        "\n",
        "**Simple Returns Formula**\n",
        "\n",
        "$$R_{simple} = \\frac{p_{_1} - p_{_0}}{p_{_0}}$$\n",
        "\n",
        "For example, if we were calculating yearly returns and on day 1, the portfolio was worth \\\\$100 and at the end of the year it was worth \\\\$125, the simple return would be (125-100)/100 = 0.25 = 25% gain.\n",
        "\n",
        "\n",
        "**Log Returns Formula**\n",
        "\n",
        "$$R_{log} = ln \\Big( \\frac{p_{_1}}{p_{_0}} \\Big)$$\n",
        "\n",
        "For example, if our portfolio was worth \\\\$100 at the start of the year and \\\\$80 by the end of the year, the log return would be ln(80/100) = -0.223 = -22.3% loss.\n",
        "\n",
        "Log returns are used in this case because it is a common assumption in many financial models that returns are normally distributed, and log returns have good mathematical properties, which make them easier to work with considering this assumption.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3Vj8r39CeCm"
      },
      "source": [
        "### **1.2 Calculate Log Returns, Remove Unused Columns, and Drop Nulls**\n",
        "\n",
        "We need to remove the nulls for the weekend dates. The following code snippet calculates the log returns for the S&P 500, NASDAQ, and Bitcoin and removes the original price columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6lYoiCPCeCn"
      },
      "outputs": [],
      "source": [
        "# Remove rows with missing values\n",
        "df.dropna(inplace = True)\n",
        "\n",
        "# Calculate log returns\n",
        "df[\"SP500\"] = np.log(df[\"^GSPC\"]) - np.log(df[\"^GSPC\"].shift(1))\n",
        "df[\"NASDAQ\"] = np.log(df[\"^IXIC\"]) - np.log(df[\"^IXIC\"].shift(1))\n",
        "df[\"Bitcoin\"] = np.log(df[\"BTC-USD\"]) - np.log(df[\"BTC-USD\"].shift(1))\n",
        "\n",
        "# Remove original price columns\n",
        "df.drop([\"^GSPC\",\t\"^IXIC\", \"BTC-USD\"], axis = 'columns', inplace = True)\n",
        "\n",
        "# Remove rows with missing values (again)\n",
        "df.dropna(inplace = True)\n",
        "\n",
        "# Display the first five rows\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yei4aJgMCeCn"
      },
      "source": [
        "### **1.3 Show Summary Stats for the Index Returns**\n",
        "\n",
        "This code snippet will generate descriptive statistics of your dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rHFbiKyCeCn"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5P5G85nCeCo"
      },
      "source": [
        "## **2. Variance and Standard Deviation**\n",
        "\n",
        "Investors have to keep volatility in mind as well when choosing an investment. For example, a pension fund may need to be extra careful with its money and will want to ensure they aren't getting into any extremely volatile investments. There are also hedge funds, which short stocks and even trade volatility with options. As you can see, whether you're risk-seeking or risk-averse, the volatility (risk) of an investment is something you should care about.   \n",
        "\n",
        "A simple measure of volatility is the variance. Variance is used to see how far away each data point in a set is away from the mean. Variance is calculated with the following steps:\n",
        "\n",
        "- Take the difference between each data point and the mean\n",
        "- Square each difference so that they're all positive values\n",
        "- Sum up the squared results\n",
        "- Divide this by the count of data points minus one\n",
        "\n",
        "$$\\sigma^2 = \\frac{\\sum(x_i - \\overline{x})^2}{n-1}$$\n",
        "\n",
        "where\n",
        "\n",
        "$\\sigma^2$ = sample variance </br>\n",
        "$x_i$ = value of one observation </br>\n",
        "$\\overline{x}$ = mean of all observations </br>\n",
        "$n$ = number of observations </br>\n",
        "\n",
        "The larger the variance, the further spread out it is from the mean. Variance treats all deviations from the mean the same way, regardless of whether they are less than or greater than the mean. A variance of zero would indicate that each data point is the same.  \n",
        "\n",
        "Standard deviation is easy to calculate once you have the variance. All you have to do is take the square root of the variance:\n",
        "\n",
        "$$\\sigma = \\sqrt{\\sigma^2}$$\n",
        "\n",
        "where\n",
        "$\\sigma$ = standard deviation\n",
        "\n",
        "Standard deviation is another commonly used statistical measure to quantify market volatility. You would expect newer growth stocks to have higher standard deviations and more established blue-chip stocks to have lower standard deviations of returns. We will illustrate this by comparing daily returns from the last five years of the S&P 500 and NASDAQ, as well as Bitcoin prices. While Bitcoin isn't necessarily a growth stock, it's still very new compared to the S&P 500 and NASDAQ, so you would expect to see bigger swings in the price when compared to those two stock indices.\n",
        "\n",
        "This is illustrated below by taking the standard deviations of returns over the last five years. These results are as expected: both the S&P 500 and NASDAQ have very similar daily standard deviations. Bitcoin, on the other hand, has almost five times the standard deviation of the two stock indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spHhcIQaCeCo"
      },
      "outputs": [],
      "source": [
        "df.std()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-yFAfoJCeCo"
      },
      "source": [
        "We can also visualize this by graphing returns in Python using the plot method in pandas in conjunction with standardized y-axis limits so that we can ensure we're comparing returns on the same scale:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGqBFruGCeCo"
      },
      "outputs": [],
      "source": [
        "# Generate two plots\n",
        "ax1 = df.plot(figsize=(15, 3), y=\"SP500\", title=\"Figure 1: S&P 500 Daily Returns\")\n",
        "ax2 = df.plot(figsize=(15, 3), y=\"Bitcoin\", title=\"Figure 2: Bitcoin Daily Returns\")\n",
        "\n",
        "# Set y-axis limits for both plots\n",
        "ax1.set_ylim(-0.5, 0.4)\n",
        "ax2.set_ylim(-0.5, 0.4);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wey8-IFICeCo"
      },
      "source": [
        "The above charts show clearly how much more volatile Bitcoin is compared to the S&P 500 and hence why the variance/standard deviation of returns is much higher.\n",
        "\n",
        "Standard deviation is preferred over variance in most cases because variance is a squared result of the units of return. By taking the square root of this and thus obtaining standard deviation, the result is in the same unit as the underlying data, which in this case is returns. This intuitively makes it much easier to understand.  \n",
        "\n",
        "Keep in mind that a lower standard deviation is not necessarily preferable when considering investments. It all depends on the investor's risk preferences. A higher risk means a higher potential for rewards. Understanding the investor's perspective is key to determining what levels of risk an investor is comfortable with.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nCUlUoyCeCp"
      },
      "source": [
        "## **3. Covariance and Correlation**\n",
        "\n",
        "We are able to compare the performance of stocks that have different price levels by using returns and standard deviation. How can we look at the joint performance of two stocks? For this, we turn to covariance and correlation. We will start with covariance:\n",
        "\n",
        "$$\\frac{\\sum(x_{i}-\\overline{x})(y_{i}-\\overline{y})}{N-1}$$\n",
        "\n",
        "where\n",
        "\n",
        "$x_i$ = value of one observation of $x$  </br>\n",
        "$y_i$ = value of one observation of $y$ </br>\n",
        "$\\overline{x}$ = mean of $x$ </br>\n",
        "$\\overline{y}$ = mean of $y$ </br>\n",
        "$N$ = number of observations </br>\n",
        "\n",
        "\n",
        "**Covariance** provides us some insight into how two variables move together. A positive covariance between stock returns would indicate that when one stock goes up, so does the other and vice versa. A negative covariance would mean that the two stocks move inversely, i.e., when one goes up, the other goes down.  \n",
        "\n",
        "Looking at the covariance matrix below for our three assets, we can see that these assets have a positive relationship with each other. It's hard to understand much more than that with these numbers, given that the units are not standardized.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if1a8KQFhRzi"
      },
      "source": [
        "### **3.1 Using a Covariance Matrix**\n",
        "\n",
        "While covariance is useful for determining the direction of two variables jointly, we can turn to correlation for a more standardized version of this. For now, when discussing correlation, we will use the Pearson's correlation coefficient. The other types of correlation will be discussed in a future lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDWd7c4zCeCp"
      },
      "outputs": [],
      "source": [
        "df.cov()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEnvOc-ig89s"
      },
      "source": [
        "Here's how to interpret it:\n",
        "\n",
        " - Diagonal values: These represent the variance of each asset. For example, 0.000149 is the variance of the S&P 500 daily log returns.\n",
        " - Off-diagonal values: These represent the covariance between two different assets. For example, 0.000160 is the covariance between the daily log returns of the S&P 500 and NASDAQ.\n",
        "\n",
        "All the values are positive, indicating a positive relationship between the returns of these assets. This means that when the return of one asset is positive, the return of the other assets tends to be positive as well.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2_XHUp2CeCp"
      },
      "source": [
        "### **3.2 Using the Pearson Correlation Coefficient Formula**\n",
        "\n",
        "The Pearson correlation coefficient is a measure of the strength of a linear relationship between two variables. The formula is as follows:\n",
        "\n",
        "$$\\rho_{_{X,Y}} = \\frac{cov(X,Y)}{\\sigma_{_{X}} \\sigma_{_{Y}}}$$\n",
        "\n",
        "where\n",
        "\n",
        "$cov$ = Covariance </br>\n",
        "$\\sigma_{_{X}}$ = Standard Deviation of $X$ </br>\n",
        "$\\sigma_{_{Y}}$ = Standard Deviation of $Y$ </br>\n",
        "\n",
        "Correlation is used in statistics to quantify the degree to which two variables move in a linear relation to each other. Correlation can range from â€“1 to 1 inclusive. A correlation of 1 means perfect correlation; the variables move exactly in tandem with one another. Perfect tandem for positive correlation means that if we know variable X increases, then variable Y also increases. A correlation of -1 indicates perfect inverse correlation. This is also perfect tandem, but in the opposite direction. Here, if we know variable X increases, then variable Y decreases. A correlation of 0 indicates there is no linear distinguishable relationship between two variables; therefore, it would be impossible to make predictions of one variable given the other. In this case, if we know variable X increases, then variable Y is equally likely to increase or decrease.\n",
        "\n",
        "A benefit of correlation over covariance is that correlation is capped from -1 to 1 while covariance can be from -inf to inf. This makes covariance a harder statistic to understand intuitively. Correlation is also proportional, which will be shown in the video below.  \n",
        "\n",
        "Keep in mind that a lot of models and financial concepts assume a constant correlation, but this is rarely the case. Correlation changes over time and will likely even change if you adjust the time range from which you're measuring correlation.\n",
        "\n",
        "When comparing the correlations of the assets we've been using so far, you can see it paints a clearer picture than the covariance did."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_Xhe17qCeCp"
      },
      "outputs": [],
      "source": [
        "round(df.corr(), 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RumdYj8CeCq"
      },
      "source": [
        "All of the variables have a positive relationship, which we saw with the covariance matrix previously. Here, we can also see the strength of the relationships: the S&P 500 is strongly correlated with NASDAQ since we've obtained a 0.943 Pearson's correlation coefficient. The relationship between NASDAQ and Bitcoin is still positive but much weaker with a 0.205 Pearson's correlation coefficient.\n",
        "\n",
        "When we chart the returns between these variables, we can see evidence of the correlations above, visually:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3GJLjBzCeCq"
      },
      "outputs": [],
      "source": [
        "# Create scatter plot with regression line\n",
        "chart = sns.regplot(x=\"SP500\", y=\"NASDAQ\", data=df).set(\n",
        "    title=\"Figure 3: Daily S&P 500 Returns vs NASDAQ Returns\"\n",
        ")\n",
        "\n",
        "# Add vertical line at x=0 and horizontal line at y=0\n",
        "plt.axvline(0, 0, 1, dash_capstyle=\"butt\", linestyle=\"--\", color=\"grey\")\n",
        "plt.plot([min(df.SP500), max(df.SP500)], [0, 0], linestyle=\"--\", color=\"grey\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkdjc6mRCeCq"
      },
      "source": [
        "This plot visually shows the relationship between the daily log returns of the S&P 500 and NASDAQ, including a regression line that represents the best-fit linear relationship between the two variables. The dashed lines at x=0 and y=0 help to visualize the positive and negative returns for each asset. This relationship shows nearly perfect correlation.\n",
        "\n",
        "Now let's visualize relationship between S&P 500 and Bitcoin daily returns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLMVf3EtCeCq"
      },
      "outputs": [],
      "source": [
        "# Create scatter plot with regression line\n",
        "sns.regplot(x=\"SP500\", y=\"Bitcoin\", data=df).set(\n",
        "    title=\"Figure 4: Daily S&P 500 Returns vs Bitcoin Returns\"\n",
        ")\n",
        "\n",
        "# Add vertical line at x=0 and horizontal line at y=0\n",
        "plt.axvline(0, 0, 1, dash_capstyle=\"butt\", linestyle=\"--\", color=\"grey\")\n",
        "plt.plot([min(df.SP500), max(df.SP500)], [0, 0], linestyle=\"--\", color=\"grey\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyaK6wqRCeCq"
      },
      "source": [
        "The relationship here is much more scattered even though the relationship is still slightly positive. These were charted using the seaborn `regplot()` method, which also shows confidence intervals around the regression line, as seen above.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9boxBGmCeCr"
      },
      "source": [
        "### **3.3 The Sharpe Ratio**\n",
        "\n",
        "Are there any statistics we can use to quantify not just return but also risk? For this, we turn to the Sharpe ratio.\n",
        "\n",
        "The Sharpe ratio allows an investor to understand the relationship between the return of an investment and its volatility. The formula is as follows:\n",
        "\n",
        "$$\\text{Sharpe Ratio} = \\frac{R_p - R_f}{\\sigma_p}$$\n",
        "\n",
        "where\n",
        "\n",
        "$R_p$ = Return of the portfolio </br>\n",
        "$R_f$ = Risk-free rate </br>\n",
        "$\\sigma_p$ = Standard deviation of the portfolio </br>\n",
        "\n",
        "\n",
        "In many cases with interest rates so low, investors will assume the risk-free rate to be 0, making the ratio:\n",
        "\n",
        "$$\\text{Sharpe Ratio} = \\frac{R_p}{\\sigma_p}$$\n",
        "\n",
        "Notice anything about the denominator? Yes, that's right: we use the standard deviation here to represent risk.  \n",
        "\n",
        "This measure is used as a way of scaling the return of an investment depending on how much risk is taken. In other words, the higher the standard deviation, the more the risk-weighted return is reduced. Let's use the same example as above with S&P 500 and Bitcoin daily returns over the last five years.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnNKYfy0CeCr"
      },
      "outputs": [],
      "source": [
        "# Calculates the Sharpe Ratios for both the S&P 500 and Bitcoin\n",
        "Sharpe_Ratio_SP500 = df[\"SP500\"].mean() / df[\"SP500\"].std()\n",
        "Sharpe_Ratio_Bitcoin = df[\"Bitcoin\"].mean() / df[\"Bitcoin\"].std()\n",
        "\n",
        "# Print the results\n",
        "print(\"Sharpe Ratio of S&P 500: \", round(Sharpe_Ratio_SP500, 5))\n",
        "print(\"Sharpe Ratio of Bitcoin: \", round(Sharpe_Ratio_Bitcoin, 5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xlMupt2kR1G"
      },
      "source": [
        "Based on these results, the S&P 500 has a slightly higher Sharpe Ratio (0.03699) compared to Bitcoin (0.03498). This suggests that the S&P 500 has provided a slightly better risk-adjusted return over the analyzed period.\n",
        "\n",
        "One major flaw with the Sharpe ratio is that it uses the standard deviation of returns in the denominator, which assumes that returns are normally distributed. This may not always--and is actually rarely--the case. This will be explored further in a future lesson.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW1ffVQrCeCr"
      },
      "source": [
        "## **4. Semivariance**\n",
        "\n",
        "How can we refine the Sharpe ratio to give an even better measure of risk-adjusted returns? Semivariance is the answer. Semivariance, also known as downside risk, is a more refined version of a standard deviation. Standard deviation looks at both the upside and downside risk of an investment. Most investors, unless you're trading short, care much more about the downside risk than the upside risk. In other words, if you bought a stock and are looking at the Sharpe ratio, you wouldn't want this number to be penalized for how far it moves to the upside. Most of the time, an investor will be much more concerned with the downside risk of a stock.  \n",
        "\n",
        "$$\\text{Semivariance} = \\frac{1}{n} \\sum_{r_i < \\overline{r}}^{n} (r_i - \\overline{r})^2$$\n",
        "\n",
        "where\n",
        "\n",
        "$r_i$ = value of one observation  </br>\n",
        "$\\overline{r}$ = mean of all observations  </br>\n",
        "$n$ = number of observations\n",
        "\n",
        "Semivariance can be thought of as an estimator of variance of the returns that are less than their average. This can be used to estimate the average loss a portfolio could incur, assuming normal distributions of returns.  \n",
        "\n",
        "Conversely, if we are short a security, we could still use semivariance, but this time, it would focus on the returns that are positive. If we are short, drops in the price create upside, so we would not want to penalize this volatility in our Sharpe ratio. However, if there are large deviations in the upward direction, then this will contribute to semivariance. In short, semivariance uses either the positive or negative returns.\n",
        "\n",
        "Now let's compute semivariance for both the S&P 500 and Bitcoin:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaGXJ1jOCeCr"
      },
      "outputs": [],
      "source": [
        "# Calculate the mean return for each\n",
        "sp500mean = df[\"SP500\"].mean()\n",
        "BTCmean = df[\"Bitcoin\"].mean()\n",
        "\n",
        "# Calculate semivariance for each\n",
        "sp500_semivariance = ((df[df[\"SP500\"] < sp500mean][\"SP500\"] - sp500mean) ** 2).mean()\n",
        "BTC_semivariance = ((df[df[\"Bitcoin\"] < BTCmean][\"Bitcoin\"] - BTCmean) ** 2).mean()\n",
        "\n",
        "# Print the semivariance results\n",
        "print(\"Semivariance of S&P 500: \", round(sp500_semivariance, 5))\n",
        "print(\"Semivariance of Bitcoin: \", round(BTC_semivariance, 5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YoFvScCndsx"
      },
      "source": [
        "This output shows that Bitcoin has a much higher semivariance (0.00181) compared to the S&P 500 (0.00021). This indicates that Bitcoin has experienced significantly larger negative deviations from its average return, suggesting higher downside risk."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1fe1KODCdVL"
      },
      "source": [
        "## **5. How Are Stock Returns Distributed?**\n",
        "\n",
        "Many models and theories surrounding stocks assume a normal distribution. We will try to determine that here with a data-based analysis. Properties of a Gaussian distribution are as follows:\n",
        "\n",
        "* Mean, median, and mode are all the same.\n",
        "* The data is symmetrical, meaning there are equal counts of observations on both sides of the mean.\n",
        "* In normally distributed data, 68.25% of all cases fall within +/- one standard deviation from the mean, 95% of all cases fall within +/- two standard deviations from the mean, and 99.7% of all cases fall within +/- three standard deviations from the mean.\n",
        "\n",
        "Let's start by pulling 20 years of daily price data for the S&P 500. We'll use similar methods we've used in the last few lessons to pull this data and will calculate the log returns here.\n",
        "\n",
        "One quick way of doing this is to determine how many data points we have on either side of the mean here. We have a bit more than 5,000 data points. The below code takes the count of data points greater than the mean and divides it by the total number of data points. This will give us the percentage of data points greater than the mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBK3uHHUCdVM"
      },
      "outputs": [],
      "source": [
        "# Starting and end dates\n",
        "start = datetime.date(2004, 8, 1)\n",
        "end = datetime.date(2024, 8, 1)\n",
        "\n",
        "# Get the data\n",
        "prices = pd.DataFrame(yfin.download([\"^GSPC\"], start, end, auto_adjust = False)[\"Adj Close\"])\n",
        "\n",
        "# Rename column to make names more intuitive\n",
        "prices = prices.rename(columns={\"^GSPC\": \"SP500\"})\n",
        "df = np.log(prices) - np.log(prices.shift(1))\n",
        "df = df.iloc[1:, 0:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsR1kEpiCdVM"
      },
      "source": [
        "### **5.1 Are Returns Symmetric?**\n",
        "\n",
        "One quick way of doing this is to determine how many data points we have on either side of the mean here. We have a bit more than 5,000 data points here. The below code takes the count of data points greater than the mean and divides it by the total number of data points. This will give us the percentage of data points greater than the mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHDtF5B5CdVN"
      },
      "outputs": [],
      "source": [
        "(len(df[df.SP500 > df.SP500.mean()])) / (len(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVBwz-geCdVN"
      },
      "source": [
        "The output will be a number between 0 and 1, representing the proportion of S&P 500 daily returns that are greater than the mean return. This can give you an idea of the skewness of the return distribution. A value close to 0.5 suggests a roughly symmetrical distribution, while a value significantly greater than 0.5 suggests a negative skew (more returns below the mean).\n",
        "\n",
        "We're getting about 52.6% of data points being greater than the mean, which shows we have a slightly negative skew to this dataset. We can't rule out symmetric returns based on this since it is only a sample of data and is reasonably close to the 50% mark. This makes it hard to say for certain whether S&P 500 returns are symmetric or not, but it is still a reasonable assumption to make here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG-3rnkiCdVN"
      },
      "source": [
        "### **5.2 Is Volatility Constant?**\n",
        "\n",
        "The following code calculates and plots the rolling 50-day standard deviation of the S&P 500 log returns, providing a visualization of the volatility over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQ5dkm2ICdVN"
      },
      "outputs": [],
      "source": [
        "# Calculate rolling standard deviation\n",
        "vols = pd.DataFrame(df.SP500.rolling(50).std()).rename(columns={\"SP500\": \"S&P 500 STD\"})\n",
        "\n",
        "# set figure size and plot rolling standard deviations\n",
        "plt.figure(figsize=(12, 5))\n",
        "sns.lineplot(\n",
        "    x=\"Date\",\n",
        "    y=\"S&P 500 STD\",\n",
        "    data=vols,\n",
        "    label=\"S&P 500 50 day standard deviation rolling avg\",\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8nkQT-lCdVO"
      },
      "source": [
        "From the above plot, it can be clearly seen that volatility is anything but constant. This adds another layer of complexity to modeling stock returns, especially in the many models which assume constant volatility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FmXRxmtCdVO"
      },
      "source": [
        "## **6. Are Stock Returns Normally Distributed?**\n",
        "\n",
        "The normal distribution is one of the most common distributions used in modeling random variables. Indeed, many phenomena in the natural and social sciences can be modeled by normal distribution. One of the great advantages of the normal distribution is its simplicity. We can completely describe a normal distribution through two numbers: one for the center of the distribution and one for the uncertainty about that center. The first number refers to the mean, and the second number refers to the standard deviation.\n",
        "\n",
        "Once we have these two numbers, we can draw inferences, estimate percentiles, compute probabilities that a point falls within a region, and more. If our data is well represented by the normal distribution, then we can confidently use the mean and standard deviation to report our portfolio expected returns and volatilities. If our data is not well represented by the normal distribution, then we need to find other distributions that are more suitable. Thus, when we have a distribution of stock returns, for example, we'll want to start this assessment by visualizing the returns to see if they appear to be normal. Of course, we can follow this up with more quantitative assessments by running statistical tests.\n",
        "\n",
        "We can visualize the data using the `hist()` method. We pass in `bins = 100` as a parameter to determine the amount of buckets to place the data in. The more bins you have, the more granular the data will look in a histogram. Increasing the bins too much may result in slightly noisy data, which will make it tougher to determine a normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdoRxLomCdVP"
      },
      "outputs": [],
      "source": [
        "df.hist(bins=100);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Iso8Znrqe74"
      },
      "source": [
        "The chart above looks like it could be normally distributed, but we need to be a little more scientific to determine if that's actually the case or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDVBfbSZCdVP"
      },
      "source": [
        "### **6.1 Conducting a Normality Test**\n",
        "\n",
        "We can use the `normaltest()` method to determine if the sample data could fit a normal distribution. This method uses D'Agostino and Pearson's normality test, which combines skew and kurtosis to produce an omnibus test of normality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0c6JIw9CdVP"
      },
      "outputs": [],
      "source": [
        "stats.normaltest((np.array(df.SP500)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NetxcwY8CdVP"
      },
      "source": [
        "The output of this code includes two values:\n",
        "\n",
        " - Statistic: A test statistic that measures the deviation from normality. Higher values indicate a greater deviation.\n",
        " - p-value: The probability of observing the data if it were truly normally distributed. A small p-value (typically less than 0.05) suggests that the data is not normally distributed.\n",
        "\n",
        "The output above shows the results of the D'Agostino and Pearson normality test on our S&P 500 log return data. The statistic of 1234.73 is very high, indicating a substantial deviation from a normal distribution. The extremely small p-value (7.61e-269) provides strong evidence to reject the null hypothesis that the data is normally distributed. In simpler terms, this test strongly suggests that the S&P 500 daily log returns in our dataset do not follow a normal distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omnZwIfhCdVQ"
      },
      "source": [
        "### **6.2 Testing Skewness and Kurtosis**\n",
        "\n",
        "As one added testing step, we can test the skewness and kurtosis of our distribution using the Jarque-Bera test. The test statistic will always be greater than zero. The further the test statistic is from zero, the more likely the sample data does not match a normal distribution.\n",
        "\n",
        "Lucky for us, Python has another library for us to use here, which really simplifies the analysis. From the `scipy.stats` library, we can apply the `jarque_bera()` method directly to our data to get the test statistic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHFPOCMgCdVQ"
      },
      "outputs": [],
      "source": [
        "stats.jarque_bera((np.array(df.SP500))).pvalue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPwMW_bVCdVQ"
      },
      "source": [
        "A small p-value (typically less than 0.05) indicates that the data is likely not normally distributed. This output indicates that the p-value from the Jarque-Bera test is 0.0 (or extremely close to zero). This strongly suggests that the S&P 500 daily log returns in your dataset are not normally distributed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrWyj_SJCdVQ"
      },
      "source": [
        "### **6.3 Where Does Our Gaussian Distribution Break Down?**\n",
        "\n",
        "According to the normality test, our data is not normally distributed despite the histogram looking like it may be. So why does the data fail the normality test? The answer likely comes down to fat tails. Fat tails essentially mean that extreme events (+/-3 standard deviations away from the mean) are more likely than the normal distribution would imply.\n",
        "\n",
        "To determine how many standard deviations away from the mean a specific number is, we need to use\n",
        "\n",
        "$$\\frac{X - \\bar{X}}{\\text{Sample standard deviation}}$$\n",
        "\n",
        "Let's do this for the min and max of the sample data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxg3mlnICdVQ"
      },
      "outputs": [],
      "source": [
        "# minimum and maximum daily log returns\n",
        "dfMax = df.SP500.max()\n",
        "dfMin = df.SP500.min()\n",
        "\n",
        "# Print maximum and minimum daily log returns\n",
        "print(\"Maximum return of sample data is: \", round(dfMax, 5))\n",
        "print(\"Minimum return of sample data is: \", round(dfMin, 5))\n",
        "print(' - - - - - - - - - -')\n",
        "\n",
        "# Calculates the number of standard deviations from the mean return\n",
        "num_dev_max = (df.SP500.max() - df.SP500.mean()) / df.SP500.std()\n",
        "num_dev_min = (df.SP500.min() - df.SP500.mean()) / df.SP500.std()\n",
        "\n",
        "# Print num_dev_max and num_dev_min\n",
        "print(\"Number of standard deviations from the mean for the maximum return: \", round(num_dev_max, 5))\n",
        "print(\"Number of standard deviations from the mean for the minimum return: \", round(num_dev_min, 5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr1ysBRsCdVS"
      },
      "source": [
        "This output provides valuable insights into the distribution of S&P 500 daily log returns, particularly highlighting the presence of outliers or fat tails:\n",
        "\n",
        " - Maximum and Minimum Returns: The maximum daily return of approximately 10.957% and the minimum daily return of approximately -12.765% show the range of returns observed in our data.\n",
        " - Z-scores: The z-scores for the maximum and minimum returns are 9.05 and -10.6, respectively. These are extremely high z-scores, indicating that both the maximum and minimum returns are significant outliers, far away from the mean in terms of standard deviations.\n",
        "\n",
        "In a normal distribution, you would rarely observe data points more than 3 standard deviations away from the mean. These high z-scores suggest that the S&P 500 daily returns have fatter tails than a normal distribution, meaning extreme events (large positive or negative returns) are more likely than would be expected under normality.\n",
        "\n",
        "This information is crucial for risk management and modeling, as relying on the assumption of normality can lead to underestimating the probability of extreme events and potential losses.\n",
        "\n",
        "These standard deviations are humongous when compared to the normal distribution. We can see this analytically when we plug in the z score to the `norm.cdf()` method to determine the probability this value could be in a normal distribution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUlblqLkCdVS"
      },
      "outputs": [],
      "source": [
        "stats.norm.cdf(-10.60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCdiUT_VCdVS"
      },
      "source": [
        "This implies that the chance we could have a move as small as -12.77% is 1.49e-26. This probability is so low that we would never expect an event like this to happen in our lifetime. We have multiple events like this, as illustrated by the minimum and maximum.\n",
        "\n",
        "Going further with this idea, based on normal distribution z tables, we would expect 99.7% of our data points to be within +/- 3 standard deviations from the mean. Let's determine this for our sample data. First off, we need to find the cut-off values at +/- 3 standard deviations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzhMHMrVCdVS"
      },
      "outputs": [],
      "source": [
        "# Calculates the upper and lowers bounds\n",
        "upper = (3 * df.SP500.std()) + df.SP500.mean()\n",
        "lower = (-3 * df.SP500.std()) + df.SP500.mean()\n",
        "\n",
        "# Print the results\n",
        "print(\"Upper bound: \", round(upper, 5))\n",
        "print(\"Lower bound: \", round(lower, 5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hPB7TBzCdVU"
      },
      "source": [
        "The above two calculations would imply that 99.7% of all of our data points should be in between -0.0359 and 0.03654.\n",
        "\n",
        "Since we have 5,031 data points, we would expect about 15 (i.e., 3% of 5,031) of them to be outside of that range if normality was held. Now let's see how many we actually have. Let's filter the `df` DataFrame to select rows where the S&P 500 daily log returns are outside the range of upper bound and lower bound. Then, we'll count the number of the data points that fall in this range:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpI05MTCCdVU"
      },
      "outputs": [],
      "source": [
        "# Calculates the number of data points\n",
        "len(df[(df[\"SP500\"] < lower) | (df[\"SP500\"] > upper)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A4qFnBkCdVV"
      },
      "source": [
        "That's a significant number of outliers considering that in a normal distribution, you would expect only about 0.3% of the data points to fall outside the range defined by 3 standard deviations from the mean.\n",
        "\n",
        "This finding further supports the conclusion from the normality tests that the S&P 500 daily log returns are not normally distributed and exhibit fat tails. The presence of these outliers highlights the importance of considering alternative distributions and risk measures when modeling and analyzing financial data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95C8VuEZCdVV"
      },
      "source": [
        "## **7. Non-Gaussian Distributions**\n",
        "\n",
        "One potential alternative distribution we could use to forecast stock returns is the Student's t-distribution. This is very similar to a normal distribution except it has heavier tails. Theoretically, this sounds perfect for daily returns based on what we've seen up to this point.\n",
        "\n",
        "Let's proceed with a visual inspection of the distribution of our data by superimposing the normal distribution on the kernel density estimation (KDE) of S&P 500 returns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0ZeuLfyRvw0"
      },
      "outputs": [],
      "source": [
        "# Sampling from normal distribution\n",
        "np.random.seed(222)\n",
        "normal_dist = stats.norm.rvs(size=len(df[\"SP500\"]), loc = df[\"SP500\"].mean(), scale = df[\"SP500\"].std())\n",
        "\n",
        "# Creating an additional column in df in order to use the KDE plot functionality of pandas\n",
        "df['Normal Sample'] = normal_dist\n",
        "\n",
        "# Plotting the KDE plots\n",
        "df[['SP500', 'Normal Sample']].plot(kind = 'kde', xlim = (-0.1, 0.1), figsize = (12,4));\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tge68c0leGMs"
      },
      "source": [
        "The S&P 500 returns seem a lot more leptokurtic. A leptokurtic distribution has fatter tails and a sharper peak than a normal distribution. By definition, leptokurtic distributions have kurtosis greater than 3.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STPDMEBCebIq"
      },
      "outputs": [],
      "source": [
        "df.SP500.kurt()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RUgruN2ejTb"
      },
      "source": [
        "Indeed, the kurtosis of the S&P 500 is significantly greater than 3, confirming that the distribution of S&P 500 daily log returns is highly leptokurtic.\n",
        "\n",
        "Now, let's check if the tails of the S&P 500 are also fatter than those of a normal distribution. The following code creates a kernel density estimation (KDE) plot to compare the tails of the actual S&P 500 daily log returns with the tails of the generated normal sample. It focuses on the left tail (negative returns) by setting the x-axis limits to (-0.075, -0.04)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3zgE6yjeieA"
      },
      "outputs": [],
      "source": [
        "# Observing the tails\n",
        "df[['SP500', 'Normal Sample']].plot(kind = 'kde', xlim = (-0.075, -0.04), ylim = (-1, 2), figsize = (12,4));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0YdA0zPf0Tn"
      },
      "source": [
        "The resulting plot visually shows the differences in the tails of the two distributions. We observe that the actual S&P 500 returns have a fatter left tail than the normal distribution, indicating a higher probability of large negative returns. This visualization provides further evidence of the non-normality of the S&P 500 returns and the presence of fat tails.\n",
        "\n",
        "At this stage, we will calibrate the parameters of the Student's t-distribution using Maximum Likelihood Estimation (MLE) to align the distribution closely with our observed data. The following code fits a Student's t-distribution to the S&P 500 daily log returns using [Maximum Likelihood Estimation (MLE)](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.fit.html) and then creates a KDE plot to compare the fitted t-distribution with the actual data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkVFYbGWSOXc"
      },
      "outputs": [],
      "source": [
        "# Fit the t-distribution using MLE\n",
        "params = stats.t.fit(df.SP500)\n",
        "\n",
        "# We plot the fitted distribution against the kde of the data\n",
        "df['t-Sample *Fitted*'] = stats.t.rvs(*params, size = len(df))\n",
        "df[['SP500', 't-Sample *Fitted*']].plot(kind = 'kde', figsize = (12,4), xlim = (-0.1, 0.1));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hv8Z-nFEh5Xp"
      },
      "source": [
        "From the visual inspection, it's evident that the synthetic data generated from the fitted Student's t-distribution offers a more accurate approximation of our actual data compared to what we get from a normal distribution.\n",
        "\n",
        "Let's check the tails once more. We will again zoom in to observe proximity between the left tail of the actual S&P 500 daily log returns with the left tail of the fitted Student's t-distribution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyUJWMmwgTeL"
      },
      "outputs": [],
      "source": [
        "# Plot left tail area\n",
        "df[['SP500', 't-Sample *Fitted*']].plot(kind = 'kde', figsize = (12,4), xlim = (-0.075, -0.04), ylim = (-1, 2));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtuyA6nV7j9u"
      },
      "source": [
        "This time, the tails seem to be better explained by the t-distribution. Here, we can observe that the fitted t-distribution captures the fat tail of the S&P 500 returns more accurately than the normal distribution. This further demonstrates the suitability of the t-distribution for modeling financial data that exhibits fat tails and the potential for extreme events."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}